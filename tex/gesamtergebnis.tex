\chapter{Gesamtergebnis \small{(Stach)}}

Für den produktiven Einsatz unseres Crawlers haben wir uns für die in Python entwickelte Variante entschieden.
Das Scrapy Framework bietet bei weitem die meisten Möglichkeiten und ist leicht erweiterbar.
In JavaScript und Rust bestehen zwar Libraries zum entwickeln einen Crawlers, ein komplette Framework was einem
viele Aufgabe abnehmen kann, existiert jedoch nicht.

Lezteres haben wir noch das Crawling mit dem Preprocessing vereint. Durch die Entwicklung einer PreProcessing-Pipeline
werde jetzt die Ergebnisse des Crawl-Vorgangs, bevor Sie in die Datenbank geschrieben werden, noch durch den von uns
entwickelten Preprocessing Algorithmus behandelt. Dadurch ergibt sich ein Endergebnis von ca. 10.000 gecrawlten und preprocessten
Polizeiberichten in der MongoDB. Folgend ein Beispiel eines Polizeiberichts:


\begin{minted}[xleftmargin=20pt,linenos,breaklines,fontsize=\small]{json}
{
  "_id": {
    "$oid": "5bf97a29ca8c53043dcabe4d"
  },
  "category": "Friedrichshain-Kreuzberg",
  "timestamp": {
    "$date": "2018-11-24T16:30:00.000+0000"
  },
  "title": "Auseinandersetzung zwischen zwei Gruppen",
  "url": "https://www.berlin.de/polizei/polizeimeldungen/pressemitteilung.760957.php",
  "text": "...",
  "text_pre_processed_v1": ["..."],
  "text_pre_processed_v2": [["..."]],
  "text_pre_processed_v3": [[0, 14, "..."]],
  "text_pre_processed_v4": [[[0, 14, "..."]]],
  "updated": {
    "$date": "2018-11-24T17:43:22.896+0000"
  },
  "created": {
    "$date": "2018-11-24T17:19:53.191+0000"
  }
}
\end{minted}
