\chapter{Einleitung \small(Laas)}

\section{Grundlagen Suchmaschinen}
Um eine eigene Suchmaschine zu implementieren, gilt es zunächst, Grundlagen bestehender Suchsysteme zu verstehen. Denn die bekannten Suchmaschinen wie Google, Bing oder Yahoo! haben sich in den vergangenen 20 Jahren stark etabliert, bei einigen Nutzern bis hin zur Unentbehrlichkeit beim Surfen im Internet. Dabei haben sich bestimmte Grundfunktionen durchgesetzt. An diese haben sich die Nutzer gewöhnt und erwarten sie auch bei ähnlichen Diensten. Diese Arbeit beschränkt sich zunächst auf bekannte Erhebungs-, Speicher- und Aufbereitungs-Techniken, um zu verstehen, wie eine funktionierende Eigenimplementation zu erreichen ist.\\
Wird allgemein von Suchmaschinen gesprochen, sind in der Regel Web-Suchmaschinen wie Google gemeint. Sie haben das Ziel, möglichst viele öffentlich zugänglichen Informationen zu erfassen und damit eine große Zahl von Nutzern zu befriedigen. Diese Art wird als horizontale Suchmaschine bezeichnet, da sie Informationen aus so vielen Themengebieten wie möglich erfasst.\\
Spezialsuchmaschinen hingegen beschränken sich bewusst auf einen Anwendungsbereich. Sie werden auch als vertikale Suchmaschinen bezeichnet, können aufgrund ihrer Spezialisierung also auch Inhalte erfassen, die ihrem vorher genannten Pendant möglicherweise nur eingeschränkt zugänglich sind oder gar gänzlich verschlossen bleiben (etwa Deep Web oder dynamische Inhalte). Zudem erlauben sie dem Nutzer oft eine viel genauere Recherche, da das Ranking bei der Suchanfrage auf die Art der Dokumente angepasst werden kann.

\section{Content Aquisition mit Crawlern}
Die ``Content Acquisition`` (deutsch Inhaltsbeschaffung) beschreibt das Sammeln von Inhalten in die Datenbank der Suchmaschine und damit die zentrale Funktion zur Gewinnung der Datenbasis. Klassisch geschieht dies mithilfe von Web-Crawlern. Das sind Computerprogramme, die automatisch und kontinuierlich nach neuen Inhalten suchen, jedoch auch Änderungen oder gelöschte Inhalte erkennen. Für Web-Suchmaschinen navigieren die Crawler dabei von unterschiedlichen Startpunkten über die von ihnen gefundenen Hyperlinks. Bei Spezialsuchmaschinen wird das Crawlen etwa auf bestimmte Domains oder Ordner beschränkt. Darüber hinaus gibt es auch Inhalt, welcher vom Suchmaschinenanbieter selbst generiert wird. So können gefundene Adressen etwa auf einer Karte visualisiert oder Gruppen von zusammenhängenden Dokumenten analysiert und aufbereitet dargestellt werden.

\section{Robots Exclusion Standart, Metadaten und Sitemaps}
Heutige Webseiten enthalten viele Informationen, die ein menschlicher Besucher nicht ohne Aufwand einsehen kann, da diese für die maschinelle Verarbeitung, etwa durch einen Crawler, gedacht sind. Einfachen HTML-Tags (meist in einem ``<meta>``-Block) können wichtige Meta-Daten über ein Dokument enthalten. Bereits zur Inhaltsbeschaffung können also für die spätere Indexierung nützliche Informationen gesammelt werden. Dieser Ansatz hat sich im World Wide Web jedoch aufgrund von Missbrauch nicht durchgesetzt und findet heute nur noch selten Anwendung. Bei speziellen Dokumenten nutzt etwa Google die Meta-Daten aus HTML-Tags für die generierung von Text-Snippets, welche bei Suchergebnisse angezeigt werden.\\
Ein deutlich gebräuchlicheren Ansatz stell der seit Anfang der 90er Jahre benutzte ``Robots Exclusion Standart`` dar. Er ist, anders als es dar Name vermuten lässt, jedoch nur eine Übereinkunft der Suchmaschinenbetreiber, welcher heute als Quasistandart gilt. Schon mit dem Aufkommen der ersten Web-Crawler bemerkte man, dass diese häufig für Engpässe oder gar Ausfälle (Denial of Service) bei der Verfügbarkeit von Webservern verantwortlich waren. Man entschloss sich deshalb zur Nutzung einer Textdatei mit dem Namen ``robots.txt``, welche in einem einheitlichen Format im Stammverzeichnis einer Domain angelegt und mit Anweisungen und Regeln an Web-Crawler gefüllt wird, darunter etwa welche Inhalte vom Crawler nicht besucht werden sollen. Auch können für unterschiedliche Crawler unterschiedliche Regeln definiert werden, bis hin zum kompletten Ausschluss.\\
Immer mehr Verbreitung findet die Nutzung von XML-Sitemaps. Diese können als Weiterentwicklung der robots.txt angesehen werden und bestehen aus einer Liste aller Inhalte einer Domain, welche von Crawler analysiert werden können. Dazu können pro Dokument Informationen angegeben werden, wie etwa eine Priorität, das letzte Änderungsdatum oder die Häufigkeit von Veränderungen an diesem Dokument.\\
Die drei genannten Möglichkeiten eines Webseitenbetreibers, Einfluss auf fremde Crawler zu nehmen, sind dabei für beide Seiten nützlich. Werden Meta-Tags, robots.txt oder Sitemap regelmäßig gepflegt, kann der Inhalteanbieter davon ausgehen, dass alle seine Inhalte von den Crawlern erfasst werden und somit in der jeweiligen Suchfunktion gefunden werden können. Außerdem kann er bestimmte Bereiche bewusst vor Zugriffen sperren, etwa wenn diese keine relevanten Daten enthalten. Somit wird sein Webserver vor unnötigen Zugriffen verschont, womit Ressourcen eingespart werden. Nützlich ist auch das ausschließen von Dokumenten in denen sich Namen oder Kontaktadressen befinden. Diese sind in der Regel für den Besucher der Website gedacht, sollen jedoch nicht für Werbezwecke oder ähnliches gesammelt werden. Die Vorteile des Suchmaschinenbetreibers sind ähnliche: Auch er kann Ressourcen sparen, indem er gezielt Dokumente mit einer hohen Priorität anfragt und sich gleichzeitig merkt, wann er das nächste Mal nach einer Aktualisierung schauen sollte. Außerdem kann er seinen Nutzer vollständigere Suchergebnisse liefer, die er ohne Hilfe möglicherweise nicht anbieten könnte.\\
Abschließend muss jedoch bedacht werden, dass es keine bindenden Regeln für Webcrawler gibt. Generell gibt es somit auch keinen Schutz vor Crawlern, worüber sich jeder Anbieter von öffentlich zugänglichen Informationen im klaren sein sollte. Vielmehr sind die genannten Techniken, wie bereits angesprochen, eine Übereinkunft von welcher beide Seiten profitieren können, jedoch nicht müssen.

\section{Index}
Im Vorherigen bereits genannt wurde der Index einer Suchmaschine. Dieser enthält Informationen und Referenzen zu allen bekannten Dokumente, welche etwa mittels eines Crawlers erfasst wurden. Bei horizontalen Suchmaschinen gibt es häufig neben einem allgemeinen Web-Index eine Vielzahl von Kollektionen, die je einen Themen- oder Mediumsbereich abdecken, etwa für Bilder, Videos oder Nachrichten. Dem Nutzer bietet dies den Vorteil, Suchanfragen auch gezielt an eine Kollektion stellen zu können. Allgemeine Anfragen hingegen können dem Web-Index und mehrere Kollektionen gleichzeitig gestellt werden. Für diese Art der Datenbasis bedarf es jedoch auch einem erhöhten Aufwand bei der Vorauswahl und der Verarbeitung. Häufig werden hierbei mehrere spezialisierte Crawler eingesetzt, was als ``focused crawling`` bezeichnet wird. Vertikale Suchdienste haben es hier leichter, da sie sich von vornherein auf einen Bereich begrenzen. Sie halten deshalb in der Regel eine viel kleinere Zahl von Kollektionen, bis hin zu einem einzigen Index.\\
Vor allem bei den größten Suchmaschinen gibt es keinen einzelnen Index-Standort, sondern vielmehr mehrere verteilte Indexe. Dies bietet bei Kunden auf der gesamten Welt nicht nur den Vorteil von niedrigeren Latenzen, sondern ermöglicht auch eine schnellere Aktualisierung. Denn werden neue, veränderte oder gelöschte Dokumente von den Crawlern erfasst, muss auch der Index angepasst werden, um etwa die Nutzer nicht auf nicht mehr existierende Inhalte zu verweisen.

\subsection{Indexer}
Für den Aufbau des Index zuständig ist der sogenannte Indexer. Dieses Computerprogramm bereitet hierfür die gefundenen Dokumente mit Werkzeugen der Syntaxanalyse auf, welche suchmaschinenunabhängig im Bereich des Information Retrievals genutzt werden. Häufig erstellt der Indexer dabei einen ``invertierten`` Index, für welchen die Dokumente zunächst in kurze Textabschnitte von einem einzelnen Wort bis hin zu einem vollständigen Satz zerlegt werden. Diese werden anschließend mit einem Verweisen auf das entsprechende Dokument in den Index aufgenommen, kommen dabei nur einmal vor und halten deshalb bei mehrfachem Vorkommen in gleichen oder unterschiedlichen Dokumenten mehrere Verweise. Verglichen werden kann ein invertierter Index also mit Stichwortverzeichnissen, jedoch beinhaltet der Index alle vorkommenden Wörter. Diese Art der Indexierung hat den Vorteil, dass bei Suchanfragen nicht alle Dokumente komplett durchsucht werden müssen, was Ressourcen spart. Da nun jedoch nur Repräsentationen der Dokumente durchsucht werden, ist es umso wichtiger einen gut funktionierenden Indexer zu betreiben, da ansonsten die erfassten Dokumente von den Nutzern unter Umständen gar nicht gefunden werden können.\\
Noch etwas aufwändiger sind komplexe invertierte Indexe. Sie enthalten nicht nur Verweise auf die Dokumente, sondern auch die Position und die Anzahl des Vorkommens innerhalb eines Dokuments. Diese Informationen sind besonders für das spätere Ranking bei Suchanfragen wichtig.