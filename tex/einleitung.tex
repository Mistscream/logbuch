\chapter{Einleitung \small(Laas)}

\section{Grundlagen Suchmaschinen}
Um eine eigene Suchmaschine zu implementieren gilt es zunächst, Grundlagen bestehender Suchsysteme zu verstehen. Denn die bekannten Suchmaschinen wie Google, Bing oder Yahoo! haben sich in den vergangenen 20 Jahren stark etabliert, bei einigen Nutzern bis hin zur Unentbehrlichkeit. Dabei haben sich bestimmte Grundfunktionen durchgesetzt. An diese haben sich die Nutzer gewöhnt und erwarten sie auch bei ähnlichen Diensten. Beschränkt werden soll sich in dieser Arbeit jedoch zunächst auf verwendete Techniken, um zu verstehen, wie Daten erhoben und aufbereitet werden müssen, um eine funktionierende Eigenimplementation zu erreichen.\\
Wird allgemein von Suchmaschinen gesprochen, ist in der Regel eine Web-Suchmaschine wie Google gemeint. Sie haben das Ziel möglichst viele öffentlich zugänglichen Informationen zu erfassen und damit eine große Zahl von Nutzern zu befriedigen. Diese Art wird als horizontale Suchmaschine bezeichnet, da sie Informationen so weit gefächert wie möglich erfasst.\\
Spezialsuchmaschinen hingegen beschränken sich bewusst auf einen Anwendungsbereich. Sie werden auch als vertikale Suchmaschinen bezeichnet, können aufgrund ihrer Spezialisierung also auch Inhalte erfassen, die ihrem vorher genannten Pendant nur eingeschränkt zugänglich sind oder gar gänzlich verschlossen bleiben (etwa Deep Web oder dynamische Inhalte). Zudem erlauben sie dem Nutzer oft eine viel genauere Recherche, da das Ranking bei der Suchanfrage auf die Art der Dokumente angepasst werden kann.

\section{Content Aquisition mit Crawlern}
Die Content Acquisition (deutsch Inhaltsbeschaffung) beschreibt das Sammeln von Inhalten in die Datenbank der Suchmaschine und damit die zentrale Funktion zur Gewinnung der Datenbasis. Klassisch geschieht dies mithilfe von Web-Crawlern. Das sind Computerprogramme, die automatisch und kontinuierlich nach neuen Inhalten suchen, jedoch auch Änderungen oder gelöschte Inhalte erkennen. Für Web-Suchmaschinen navigieren die Crawler dabei von unterschiedlichen Startpunkten über die von ihnen gefundenen Hyperlinks. Bei Spezialsuchmaschinen wird das Crawlen etwa auf bestimmte Domains oder Ordner beschränkt. Darüber hinaus gibt es auch Inhalt, welcher vom Suchmaschinenanbieter selbst generiert wird. So können gefundene Adressen etwa auf einer Karte visualisiert oder Gruppen von zusammenhängenden Dokumenten analysiert und sinnvoll aufbereitet dargestellt werden.

\section{Robots Exclusion Standart, Metadaten und Sitemaps}
Heutige Webseiten enthalten viele Informationen, die ein menschlicher Besucher nicht ohne Aufwand einsehen kann, da diese für die maschinelle Verarbeitung auch oder vor allem durch Web-Crawler gedacht sind. So können bereits mit einfachen HTML-Tags (etwa ``<description>``) wichtige Meta-Daten an den Crawler übergeben werden.\\
Bereits seit Anfang der 90er Jahre findet die als ``Robots Exclusion Standart`` bekannte Übereinkunft der Suchmaschinenbetreiber Anwendung, welche heute als Quasistandart gilt. Schon mit dem Aufkommen der ersten Web-Crawler bemerkte man, dass diese häufig für Engpässe oder gar Ausfälle (Denial of Service) bei der Verfügbarkeit von Webservern verantwortlich waren. Früh entschloss man sich deshalb zur Nutzung einer ``robots.txt``, welche in einem einheitlichen Format im Stammverzeichnis einer Domain angelegt und mit Anweisungen und Regeln an Web-Crawler gefüllt wird, darunter etwa welche Inhalte vom Crawler nicht besucht werden sollen. Auch können für unterschiedliche Crawler unterschiedliche Regeln definiert werden, bis hin zum kompletten Ausschluss.\\
Immer mehr Verbreitung findet die Nutzung von XML-Sitemaps. Sie kann als Weiterentwicklung der robots.txt angesehen werden und bestehen aus einer Liste aller Inhalte der Domain, welche von Crawler analysiert werden können. Dazu kann jeweils etwa eine Priorität, das letzte Änderungsdatum oder die Häufigkeit von Änderungen angegeben werden.\\
Die drei genannten Möglichkeiten für Webseitenbetreiber Informationen und Befehle an Crawler weiterzugeben sind für beide Seiten meistens nützlich. Der Suchmaschinenanbieter kann etwa Ressourcen sparen, indem er nur die Inhalte mit der höchsten Priorität erfasst und gleichzeitig direkt festlegen kann, wann er das nächste Mal nach einer Änderung schauen sollte. Zudem erhält er ohne eine genauere Analyse Meta-Daten zu erhobenen Dokumenten, welche für die Indexierung verwendet werden können. Der Inhalte-Bereitsteller kann bewusst Crawler ausschließen, um zu verhindern, dass Namen, Telefonnummern oder Mail-Adressen in Listen für Werbezwecke oder schlimmere Handlungen gesammelt werden. Zudem kann er aktiv das Ranking seiner Inhalte verbessern indem er sie mit Meta-Daten versieht oder überhaupt erst für den Crawler auffindbar macht. Abschließend sollte jedoch auch bedacht werden, dass keines der Verfahren bindend ist und ebenso gut auch einfach ignoriert werden könnte.

\section{Indexer}

\section{Repräsentation}