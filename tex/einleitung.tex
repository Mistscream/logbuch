\chapter{Einleitung \small(Laas)}

\section{Grundlagen Suchmaschinen}
Um eine eigene Suchmaschine zu implementieren gilt es zunächst, Grundlagen bestehender Suchsysteme zu verstehen. Denn die bekannten Suchmaschinen wie Google, Bing oder Yahoo! haben sich in den vergangenen 20 Jahren stark etabliert, bei einigen Nutzern bis hin zur Unentbehrlichkeit. Dabei haben sich bestimmte Grundfunktionen durchgesetzt. An diese haben sich die Nutzer gewöhnt und erwarten sie auch bei ähnlichen Diensten. Diese Arbeit beschränkt sich zunächst auf bekannte Erhebungs-, Speicher- und Aufbereitungs-Techniken, um zu verstehen, wie eine funktionierende Eigenimplementation zu erreichen ist.\\
Wird allgemein von Suchmaschinen gesprochen, sind in der Regel Web-Suchmaschinen wie Google gemeint. Sie haben das Ziel, möglichst viele öffentlich zugänglichen Informationen zu erfassen und damit eine große Zahl von Nutzern zu befriedigen. Diese Art wird als horizontale Suchmaschine bezeichnet, da sie Informationen aus so vielen Themengebieten wie möglich erfasst.\\
Spezialsuchmaschinen hingegen beschränken sich bewusst auf einen Anwendungsbereich. Sie werden auch als vertikale Suchmaschinen bezeichnet, können aufgrund ihrer Spezialisierung also auch Inhalte erfassen, die ihrem vorher genannten Pendant möglicherweise nur eingeschränkt zugänglich sind oder gar gänzlich verschlossen bleiben (etwa Deep Web oder dynamische Inhalte). Zudem erlauben sie dem Nutzer oft eine viel genauere Recherche, da das Ranking bei der Suchanfrage auf die Art der Dokumente angepasst werden kann.

\section{Content Aquisition mit Crawlern}
Die Content Acquisition (deutsch Inhaltsbeschaffung) beschreibt das Sammeln von Inhalten in die Datenbank der Suchmaschine und damit die zentrale Funktion zur Gewinnung der Datenbasis. Klassisch geschieht dies mithilfe von Web-Crawlern. Das sind Computerprogramme, die automatisch und kontinuierlich nach neuen Inhalten suchen, jedoch auch Änderungen oder gelöschte Inhalte erkennen. Für Web-Suchmaschinen navigieren die Crawler dabei von unterschiedlichen Startpunkten über die von ihnen gefundenen Hyperlinks. Bei Spezialsuchmaschinen wird das Crawlen etwa auf bestimmte Domains oder Ordner beschränkt. Darüber hinaus gibt es auch Inhalt, welcher vom Suchmaschinenanbieter selbst generiert wird. So können gefundene Adressen etwa auf einer Karte visualisiert oder Gruppen von zusammenhängenden Dokumenten analysiert und sinnvoll aufbereitet dargestellt werden.

\section{Robots Exclusion Standart, Metadaten und Sitemaps}
Heutige Webseiten enthalten viele Informationen, die ein menschlicher Besucher nicht ohne Aufwand einsehen kann, da diese für die maschinelle Verarbeitung, etwa durch einen Crawler, gedacht sind. Einfachen HTML-Tags (meist in einem ``<meta>``-Block) können wichtige Meta-Daten über ein Dokument enthalten. Bereits zur Inhaltsbeschaffung können also für die spätere Indexierung nützliche Informationen gesammelt werden. Dieser Ansatz hat sich im World Wide Web jedoch aufgrund von Missbrauch nicht durchgesetzt und findet heute nur noch selten Anwendung. Bei speziellen Dokumenten nutzt etwa Google die Meta-Daten aus HTML-Tags für die generierung von Text-Snippets, welche bei Suchergebnisse angezeigt werden.\\
Ein deutlich gebräuchlicheren Ansatz stell der seit Anfang der 90er Jahre benutzte ``Robots Exclusion Standart`` dar. Er ist, anders als es dar Name vermuten lässt, jedoch nur eine Übereinkunft der Suchmaschinenbetreiber, welcher heute als Quasistandart gilt. Schon mit dem Aufkommen der ersten Web-Crawler bemerkte man, dass diese häufig für Engpässe oder gar Ausfälle (Denial of Service) bei der Verfügbarkeit von Webservern verantwortlich waren. Man entschloss sich deshalb zur Nutzung einer Textdatei mit dem Namen ``robots.txt``, welche in einem einheitlichen Format im Stammverzeichnis einer Domain angelegt und mit Anweisungen und Regeln an Web-Crawler gefüllt wird, darunter etwa welche Inhalte vom Crawler nicht besucht werden sollen. Auch können für unterschiedliche Crawler unterschiedliche Regeln definiert werden, bis hin zum kompletten Ausschluss.\\
Immer mehr Verbreitung findet die Nutzung von XML-Sitemaps. Diese können als Weiterentwicklung der robots.txt angesehen werden und bestehen aus einer Liste aller Inhalte einer Domain, welche von Crawler analysiert werden können. Dazu können pro Dokument Informationen angegeben werden, wie etwa eine Priorität, das letzte Änderungsdatum oder die Häufigkeit von Veränderungen an diesem Dokument.\\
Die drei genannten Möglichkeiten eines Webseitenbetreibers, Einfluss auf fremde Crawler zu nehmen, sind dabei für beide Seiten nützlich. Werden Meta-Tags, robots.txt oder Sitemap regelmäßig gepflegt, kann der Inhalteanbieter davon ausgehen, dass alle seine Inhalte von den Crawlern erfasst werden und somit in der jeweiligen Suchfunktion gefunden werden können. Außerdem kann er bestimmte Bereiche bewusst vor Zugriffen sperren, etwa wenn diese keine relevanten Daten enthalten. Somit wird sein Webserver vor unnötigen Zugriffen verschont, womit Ressourcen eingespart werden. Nützlich ist auch das ausschließen von Dokumenten in denen sich Namen oder Kontaktadressen befinden. Diese sind in der Regel für den Besucher der Website gedacht, sollen jedoch nicht für Werbezwecke oder ähnliches gesammelt werden. Die Vorteile des Suchmaschinenbetreibers sind ähnliche: Auch er kann Ressourcen sparen, indem er gezielt Dokumente mit einer hohen Priorität anfragt und sich gleichzeitig merkt, wann er das nächste Mal nach einer Aktualisierung schauen sollte. Außerdem kann er seinen Nutzer vollständigere Suchergebnisse liefer, die er ohne Hilfe möglicherweise nicht anbieten könnte.\\
Abschließend muss jedoch bedacht werden, dass es keine bindenden Regeln für Webcrawler gibt. Generell gibt es keinen Schutz vor Crawlern, worüber sich jeder Anbieter von öffentlich zugänglichen Informationen im klaren sein sollte. Vielmehr sind die genannten Techniken, wie bereits angesprochen, eine Übereinkunft von welcher beide Seiten profitieren können, jedoch nicht müssen.

\section{Indexer}

\section{Repräsentation}