\chapter{Einleitung \small(Laas)}
Um eine eigene Suchmaschine zu implementieren, gilt es zunächst, Grundlagen bestehender Suchsysteme zu verstehen. Denn bekannten Suchmaschinen wie Google, Bing oder Yahoo! haben sich in den vergangenen 20 Jahren stark etabliert, bei einigen Nutzern bis hin zur Unentbehrlichkeit beim Surfen im Internet. Bestimmte Grundfunktionen haben sich dabei durchgesetzt. An diese haben sich die Nutzer gewöhnt und erwarten sie auch bei ähnlichen Diensten. Diese Arbeit soll sich dabei zunächst auf bekannte Erhebungs-, Speicher- und Aufbereitungs-Techniken beschränken.

\section{Grundlagen Suchmaschinen}
Wird allgemein von Suchmaschinen gesprochen, sind in der Regel Web-Suchmaschinen wie Google gemeint. Sie haben das Ziel, möglichst viele öffentlich zugänglichen Informationen zu erfassen und damit eine große Zahl von Nutzern zu befriedigen. Diese Art wird als horizontale Suchmaschine bezeichnet, da sie Informationen aus so vielen Themengebieten wie möglich erfasst.\\
Spezialsuchmaschinen hingegen beschränken sich bewusst auf einen Anwendungsbereich. Sie werden auch als vertikale Suchmaschinen bezeichnet, können aufgrund ihrer Spezialisierung also auch Inhalte erfassen, die ihrem vorher genannten Pendant möglicherweise nur eingeschränkt zugänglich sind oder gar gänzlich verschlossen bleiben (etwa Deep Web oder dynamische Inhalte). Zudem erlauben sie dem Nutzer oft eine viel genauere Recherche, da das Ranking bei der Suchanfrage auf die Art der Dokumente angepasst werden kann. \cite{suchmverst} \cite{boldsuche}\\
Das Ziel dieses Moduls wird, aller Voraussicht nach, die Implementierung einer vertikalen Suchmaschine sein. Begründet wird dies durch die Beschränkung auf die Einsatzberichte der Berliner Polizei.

\section{Content Aquisition mit Crawlern}
Die ``Content Acquisition`` (deutsch Inhaltsbeschaffung) beschreibt das Sammeln von Inhalten in die Datenbanken der Suchmaschine und damit die zentrale Funktion zur Gewinnung der Datenbasis. Klassisch geschieht dies mithilfe von Web-Crawlern. Das sind Computerprogramme, die automatisch und kontinuierlich nach neuen Inhalten suchen, jedoch auch Änderungen oder gelöschte Inhalte erkennen. Für Web-Suchmaschinen navigieren die Crawler dabei von unterschiedlichen Startpunkten über die von ihnen gefundenen Hyperlinks. Bei Spezialsuchmaschinen hingegen wird das Crawlen etwa auf bestimmte Domains oder Ordner begrenzt. Im vorliegenden Fall beschränken wir uns auf das Archiv der Polizeimeldungen der Berliner Polizei (siehe Kapitel \ref{ch:Python Crawler}, \ref{cha:rust} und \ref{ch: JavaScript Crawler}). \cite{suchmverst}

\subsection{Speicherung der Dokumente}
\label{sec:Speicherung}
Die gesammelten Einsatzberichte wurden zu Beginn der Arbeit vom Crawler direkt in eine MongoDB gespeichert. Diese NoSQL-Datenbank ist dokumentenorientiert und ermöglicht das Speichern von Sammlungen in JSON-ähnlichem Format, was für unsere ersten Schritte nützliche Attribute sind. Erst später wurden der Crawler mit der Programmlogik der Textaufbereitung verbunden (Erläuterung in Kapitel \ref{ch:Ergebnis}). Dieses Vorgehen bietet den Vorteil, dass die Entwicklung der Aufbereitung ständigen Zugriff auf die Daten hat und keine Abhängigkeiten besitzt. \cite{wikimongodb}\\
Da noch nicht abzusehen ist, welche Art von Index im weiteren Modulverlauf verwendet wird, kann auch noch kein Datenbank-Typ bestimmt werden. Die Auswahl dieser hängt von mehreren zu berücksichtigenden Faktoren ab, darunter die Art der Eingabe von Informationen und damit zusammenhängend, ob ein asynchroner Zugriff auf diese möglich sein soll. Auch spielt es eine Rolle, wie groß der Datenbestand einmal werden wird, ob die Daten komprimiert werden sollen und ob eine Fehlertoleranz gewünscht ist. \cite{wikiseindexing}

\section{Robots Exclusion Standart, Metadaten und Sitemaps}
Heutige Webseiten enthalten viele Informationen, die ein menschlicher Besucher nicht ohne Aufwand einsehen kann, da diese für die maschinelle Verarbeitung, etwa durch einen Crawler, gedacht sind. Einfachen HTML-Tags (meist in einem ``<meta>``-Block) können wichtige Meta-Daten über ein Dokument enthalten. Bereits zur Inhaltsbeschaffung können also für die spätere Indexierung nützliche Informationen gesammelt werden. Dieser Ansatz hat sich im World Wide Web jedoch aufgrund von Missbrauch nicht durchgesetzt und findet deshalb heute nur noch selten Anwendung. Bei speziellen Dokumenten nutzt etwa Google die Meta-Daten aus HTML-Tags für die Generierung von Text-Snippets, welche bei Suchergebnisse angezeigt werden. \cite{suchmverst} \cite{htmlmeta}\\
Ein deutlich gebräuchlicheren Ansatz stell der seit Anfang der 90er Jahre benutzte ``Robots Exclusion Standart`` dar. Er ist, anders als es dar Name vermuten lässt, jedoch nur eine Übereinkunft der Suchmaschinenbetreiber, welcher heute als Quasistandart gilt. Schon mit dem Aufkommen der ersten Web-Crawler bemerkte man, dass diese häufig für Engpässe oder gar Ausfälle (Denial of Service) bei der Verfügbarkeit von Webservern verantwortlich waren. Man entschloss sich deshalb zur Nutzung einer Textdatei mit dem Namen ``robots.txt``, welche in einem einheitlichen Format im Stammverzeichnis einer Domain angelegt und mit Anweisungen und Regeln an Web-Crawler gefüllt wird, darunter etwa welche Inhalte vom Crawler nicht besucht werden sollen. Auch können für unterschiedliche Crawler jeweils andere Regeln definiert werden, bis hin zum kompletten Ausschluss. \cite{robotsen} \cite{robotsde} \cite{suchmverst}\\
Immer mehr Verbreitung findet die Nutzung von XML-Sitemaps. Diese können als Weiterentwicklung der robots.txt angesehen werden und bestehen aus einer Liste aller Inhalte einer Domain, welche von Crawler analysiert werden können. Dazu können pro Dokument Informationen angegeben werden, wie etwa eine Priorität, das letzte Änderungsdatum oder die Häufigkeit von Veränderungen an diesem Dokument. \cite{sitemaps}\\
Die drei genannten Möglichkeiten eines Webseitenbetreibers, Einfluss auf fremde Crawler zu nehmen, sind dabei für beide Seiten nützlich. Werden Meta-Tags, robots.txt oder Sitemap regelmäßig gepflegt, kann der Inhalteanbieter davon ausgehen, dass alle seine Inhalte von den Crawlern erfasst werden und somit in der jeweiligen Suchfunktion gefunden werden können. Außerdem kann er bestimmte Bereiche bewusst vor Zugriffen sperren, etwa wenn diese keine relevanten Daten enthalten. Somit wird sein Webserver vor unnötigen Zugriffen verschont, womit Ressourcen eingespart werden. Nützlich ist auch das ausschließen von Dokumenten in denen sich Namen oder Kontaktadressen befinden. Diese sind in der Regel für den Besucher der Website gedacht, sollen jedoch nicht für Werbezwecke oder ähnliches gesammelt werden. Die Vorteile des Suchmaschinenbetreibers sind ähnliche: Auch er kann Ressourcen sparen, indem er gezielt Dokumente mit einer hohen Priorität anfragt und sich gleichzeitig merkt, wann er das nächste Mal nach einer Aktualisierung schauen sollte. Außerdem kann er seinen Nutzer vollständigere Suchergebnisse liefern, die er ohne Hilfe möglicherweise nicht anbieten könnte. \cite{suchmverst}\\
Abschließend muss jedoch bedacht werden, dass es keine bindenden Regeln für Webcrawler gibt. Generell gibt es somit auch keinen Schutz vor Crawlern, worüber sich jeder Anbieter von öffentlich zugänglichen Informationen bewusst sein sollte. Vielmehr sind die genannten Techniken, wie bereits angesprochen, eine Übereinkunft von welcher beide Seiten profitieren können, jedoch nicht müssen.\\
Die unter der Domain ``www.berlin.de`` gehostete Website der Berliner Polizei besitzt sowohl eine robots.txt als auch Sitemaps. Der robots.txt ist zu entnehmen, dass dem Crawler ``ia-archiver`` Zugriff auf alle Inhalte gegeben und dem Crawler ``zoomRank`` der Zugriff komplett verweigert wird. Alle Übrigen Programme sollen sich an eine Liste von gesperrten Adressen halten, darunter die Internetpräsenzen der Bezirksämter. Weiter verweist die robots.txt auf die verfügbaren Sitemaps. Diese umfassen mehrere Hundert Einträge mit dem Zeitpunkt der letzten Änderung, vor allem Nachrichten und Jobangebote. Das von uns betrachtete Archiv der Berliner Polizei findet nirgendwo eine Erwähnung. \cite{berlinrobots} \cite{berlinsitemap}

\section{Index}
\label{sec:Index}
Im Vorherigen bereits genannt wurde der Index einer Suchmaschine. Dieser enthält Informationen und Referenzen zu allen bekannten Dokumenten, welche etwa mittels eines Crawlers erfasst wurden. Bei horizontalen Suchmaschinen gibt es häufig neben einem allgemeinen Web-Index eine Vielzahl von Kollektionen, die je einen Mediums- oder Themenbereich abdecken, etwa für Bilder, Videos oder Nachrichten. Dem Nutzer bietet dies den Vorteil, Suchanfragen auch gezielt an eine Kollektion stellen zu können. Allgemeine Anfragen hingegen können dem Web-Index und mehrere Kollektionen gleichzeitig gestellt werden. Für diese Art der Datenbasis bedarf es jedoch auch einem erhöhten Aufwand bei der Vorauswahl und der Verarbeitung. Häufig werden hierbei mehrere spezialisierte Crawler eingesetzt, was als ``focused crawling`` bezeichnet wird. Vertikale Suchdienste haben es hier leichter, da sie sich von vornherein auf einen Bereich begrenzen. Sie halten deshalb in der Regel eine viel kleinere Zahl von Kollektionen, bis hin zu einem einzigen Index. \cite{suchmverst}\\
Vor allem bei den größten Suchmaschinen gibt es keinen einzelnen Index-Standort, sondern vielmehr mehrere verteilte Indexe. Dies bietet bei Kunden auf der gesamten Welt nicht nur den Vorteil von niedrigeren Latenzen, sondern ermöglicht auch eine schnellere Aktualisierung. Denn werden neue, veränderte oder gelöschte Dokumente von den Crawlern erfasst, muss auch der Index angepasst werden, um etwa die Nutzer nicht auf nicht mehr existierende Inhalte zu verweisen. \cite{suchmverst} \cite{wikiindex}

\subsection{Indexer}
Für den Aufbau des Index zuständig ist der sogenannte Indexer. Dieses Computerprogramm bereitet hierfür die gefundenen Dokumente mit Werkzeugen der Syntaxanalyse auf, welche suchmaschinenunabhängig im Bereich des Information Retrievals genutzt werden. Wie die Textverarbeitung eines Indexers aussehen kann, wird in Kapitel \ref{ch:Textaufbereitung} behandelt.\\
Häufig erstellt der Indexer einen ``invertierten`` Index, für welchen die Dokumente zunächst in kurze Textabschnitte von einem einzelnen Wort bis hin zu einem vollständigen Satz zerlegt werden. Diese werden anschließend mit einem Verweisen auf das entsprechende Dokument in den Index aufgenommen, kommen dabei nur einmal vor und halten deshalb bei mehrfachem Vorkommen in gleichen oder unterschiedlichen Dokumenten mehrere Verweise. Verglichen werden kann ein invertierter Index also mit Stichwortverzeichnissen, jedoch beinhaltet der Index alle vorkommenden Wörter. Diese Art der Indexierung hat den Vorteil, dass bei Suchanfragen nicht alle Dokumente komplett durchsucht werden müssen, was Ressourcen spart. Da nun jedoch nur Repräsentationen der Dokumente durchsucht werden, ist es umso wichtiger, einen gut funktionierenden Indexer zu betreiben, da ansonsten die erfassten Dokumente von den Nutzern unter Umständen gar nicht gefunden werden können. \cite{suchmverst}\\
Noch etwas aufwändiger sind komplexe invertierte Indexe. Sie enthalten nicht nur Verweise auf die Dokumente, sondern auch die Position und die Anzahl des Vorkommens innerhalb eines Dokuments. Diese Informationen sind besonders für das spätere Ranking bei Suchanfragen wichtig. \cite{suchmverst}